{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-25T11:03:13.862357Z","iopub.execute_input":"2024-02-25T11:03:13.862716Z","iopub.status.idle":"2024-02-25T11:03:14.209484Z","shell.execute_reply.started":"2024-02-25T11:03:13.862687Z","shell.execute_reply":"2024-02-25T11:03:14.208598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Upgrading and installing packages","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp\n!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-02-25T11:03:14.210956Z","iopub.execute_input":"2024-02-25T11:03:14.21133Z","iopub.status.idle":"2024-02-25T11:03:52.132343Z","shell.execute_reply.started":"2024-02-25T11:03:14.211304Z","shell.execute_reply":"2024-02-25T11:03:52.131302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading input data","metadata":{}},{"cell_type":"code","source":"train_prompt=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv')\ntest_essays=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\ntrain_essays=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\nsample_submission=pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-25T11:03:52.133878Z","iopub.execute_input":"2024-02-25T11:03:52.134225Z","iopub.status.idle":"2024-02-25T11:03:52.248251Z","shell.execute_reply.started":"2024-02-25T11:03:52.134195Z","shell.execute_reply":"2024-02-25T11:03:52.247046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Peeking input data","metadata":{}},{"cell_type":"code","source":"train_prompt.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:01:26.707247Z","iopub.execute_input":"2024-02-25T12:01:26.707635Z","iopub.status.idle":"2024-02-25T12:01:26.71752Z","shell.execute_reply.started":"2024-02-25T12:01:26.707603Z","shell.execute_reply":"2024-02-25T12:01:26.716676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_essays.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:01:30.993019Z","iopub.execute_input":"2024-02-25T12:01:30.993739Z","iopub.status.idle":"2024-02-25T12:01:31.003287Z","shell.execute_reply.started":"2024-02-25T12:01:30.99371Z","shell.execute_reply":"2024-02-25T12:01:31.002412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_essays.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:01:35.900006Z","iopub.execute_input":"2024-02-25T12:01:35.900862Z","iopub.status.idle":"2024-02-25T12:01:35.912438Z","shell.execute_reply.started":"2024-02-25T12:01:35.900823Z","shell.execute_reply":"2024-02-25T12:01:35.911471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:01:40.752602Z","iopub.execute_input":"2024-02-25T12:01:40.752964Z","iopub.status.idle":"2024-02-25T12:01:40.762737Z","shell.execute_reply.started":"2024-02-25T12:01:40.752936Z","shell.execute_reply":"2024-02-25T12:01:40.761756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras version:\", keras.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T11:03:52.319964Z","iopub.execute_input":"2024-02-25T11:03:52.320292Z","iopub.status.idle":"2024-02-25T11:04:08.18312Z","shell.execute_reply.started":"2024-02-25T11:03:52.320265Z","shell.execute_reply":"2024-02-25T11:04:08.182182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Text preprocessing required for modeling","metadata":{}},{"cell_type":"code","source":"# Text Preprocessing\nimport re\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    #Cleaning the text by replacing unwanted character with space\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuations\n    #Spliting the text based on space\n    words = text.split()\n    # Lowercase and remove non-alphabetic words\n    words = [word.lower() for word in words if word.isalpha()] \n    # Remove stop words\n    words = [word for word in words if word not in stop_words] \n    return ' '.join(words)\n\n## Cleaning the input text\ntrain_essays['clean_text'] = train_essays['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T11:04:20.06787Z","iopub.execute_input":"2024-02-25T11:04:20.068178Z","iopub.status.idle":"2024-02-25T11:04:21.191159Z","shell.execute_reply.started":"2024-02-25T11:04:20.068151Z","shell.execute_reply":"2024-02-25T11:04:21.190352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explanation:\n\nThis code preprocesses the input data to make it usable for BERT model. Here's a detailed explanation of each part:\n\n### Split Data:\nThe given code snippet begins by splitting the data into input features (`texts`) and the dependent variable (`labels`). These are extracted from a DataFrame named `train_essays`.\n\n### Initialize BERT Tokenizer:\nThe BERT tokenizer (`BertTokenizer.from_pretrained('bert-base-uncased')`) is initialized. This tokenizer will be used to convert the raw text input into tokenized sequences suitable for BERT model input.\n\n### Tokenize Texts:\nThe tokenizer is applied to the texts using the BERT model (`tokenizer(texts, padding=True, truncation=True, return_tensors='pt')`). This tokenizes the texts, adds padding to ensure uniform length, and truncates long sequences. The result is a dictionary containing the tokenized inputs suitable for the BERT model, returned as PyTorch tensors.\n\n### Convert Labels to Tensors:\nThe labels are converted into PyTorch tensors (`torch.tensor(labels)`). This step is necessary to ensure compatibility with PyTorch operations.\n\n### Split Data into Train and Validation Sets:\nThe `train_test_split` function from scikit-learn is used to split the tokenized texts and labels into training and validation sets (`train_test_split(tokenized_texts['input_ids'], labels, test_size=0.2, random_state=42)`). This function divides the dataset into a training set (80% of the data) and a validation set (20% of the data) while maintaining the class distribution.\n\n### Create PyTorch Datasets:\nPyTorch `TensorDataset` is created for both the training and validation sets (`TensorDataset(train_texts, train_labels)` and `TensorDataset(val_texts, val_labels)`). This is done to organize the tokenized texts and labels into a format that can be easily consumed by PyTorch's `DataLoader`.\n\n### Create PyTorch Dataloaders:\nPyTorch `DataLoader` objects are created for both the training and validation datasets (`DataLoader(train_dataset, batch_size=batch_size, shuffle=True)` and `DataLoader(val_dataset, batch_size=batch_size)`). These DataLoader objects are responsible for batching the data, shuffling (only for training), and loading the data efficiently during training and validation loops.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Split data into input features and dependent variable\ntexts = train_essays['clean_text'].tolist()  # Extract clean text from DataFrame\nlabels = train_essays['generated'].tolist()  # Extract labels from DataFrame\n\n# Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize texts using BERT model\ntokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n# Convert dependent variable to tensor variable\nlabels = torch.tensor(labels)\n\n# Split data into train and validation sets using train_test_split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(tokenized_texts['input_ids'],\n                                                                    labels,\n                                                                    test_size=0.2,\n                                                                    random_state=42)\n\n# Create PyTorch datasets\ntrain_dataset = TensorDataset(train_texts, train_labels)  # Create dataset for training data\nval_dataset = TensorDataset(val_texts, val_labels)  # Create dataset for validation data\n\n# Create PyTorch dataloaders\nbatch_size = 16  # Define batch size\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # DataLoader for training data\nval_loader = DataLoader(val_dataset, batch_size=batch_size)  # DataLoader for validation data","metadata":{"execution":{"iopub.status.busy":"2024-02-25T11:04:21.192248Z","iopub.execute_input":"2024-02-25T11:04:21.192539Z","iopub.status.idle":"2024-02-25T11:04:43.623556Z","shell.execute_reply.started":"2024-02-25T11:04:21.192505Z","shell.execute_reply":"2024-02-25T11:04:43.622537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Code Explanation:\n\nThis code trains a BERT model for sequence classification. Here's a detailed explanation of each part:\n\n### Initialization:\n- `best_val_loss` is initialized with a high value (`float('inf')`), and `best_model_params` is initialized as `None` to keep track of the best model parameters.\n- The BERT model for sequence classification is initialized using `BertForSequenceClassification.from_pretrained('bert-base-uncased')`.\n- AdamW optimizer is instantiated with a learning rate of `5e-5`, and Cross Entropy Loss is defined as the criterion for optimization.\n\n### Training Loop:\n- The loop iterates over the specified number of epochs (`epochs`).\n- Inside each epoch, the model is set to training mode (`model.train()`).\n- Training data is iterated over in batches using tqdm for progress visualization.\n- For each batch, the inputs and labels are moved to the appropriate device (GPU or CPU), gradients are zeroed, forward pass is performed, loss is computed, backward pass is performed, and optimizer is updated.\n- Training loss and accuracy are calculated and printed.\n\n### Validation Loop:\n- After each epoch, the model is set to evaluation mode (`model.eval()`).\n- Validation data is iterated over to calculate validation loss and accuracy.\n- Gradients are not computed during validation.\n- Validation loss and accuracy are calculated and printed.\n\n### Best Model Tracking:\n- The code tracks the best model based on validation loss.\n- If the current model has a lower validation loss than the previous best model, its parameters are saved as the best model parameters.\n\n### Output:\n- The output includes epoch-wise training and validation loss, as well as accuracy.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\n# Initialize best validation loss with a high value\nbest_val_loss = float('inf')\n# Initialize best model parameters as None\nbest_model_params = None\n\n# Define the BERT model for sequence classification\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# Define optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=5e-5)\ncriterion = nn.CrossEntropyLoss()\n\n# Training loop\n## Setting up PyTorch device as GPU if available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nepochs = 3\n\n# Loop over each epoch\nfor epoch in range(epochs):\n    ## Training the model\n    # Set the model to training mode\n    model.train()\n    ## Resetting running loss, correct predictions and total predictions for each epoch\n    running_loss = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n    \n    # Iterate over batches in the training DataLoader\n    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n        # Move inputs and labels to the appropriate device (GPU or CPU)\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs.logits, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss\n        running_loss += loss.item()\n        \n        # Calculate and update correct and total predictions for accuracy calculation\n        _, predicted = torch.max(outputs.logits, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_predictions += labels.size(0)\n    \n    # Calculate average training loss and accuracy for the epoch\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct_predictions / total_predictions\n    \n    # Validation loop\n    # Set the model to evaluation mode\n    model.eval()\n    val_running_loss = 0.0\n    val_correct_predictions = 0\n    val_total_predictions = 0\n    \n    # Iterate over batches in the validation DataLoader\n    for inputs, labels in val_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass without gradient computation\n        with torch.no_grad():\n            outputs = model(inputs)\n            loss = criterion(outputs.logits, labels)\n        \n        # Update running loss for validation\n        val_running_loss += loss.item()\n        \n        # Calculate and update correct and total predictions for validation accuracy calculation\n        _, predicted = torch.max(outputs.logits, 1)\n        val_correct_predictions += (predicted == labels).sum().item()\n        val_total_predictions += labels.size(0)\n    \n    # Calculate average validation loss and accuracy for the epoch\n    val_loss = val_running_loss / len(val_loader)\n    val_accuracy = val_correct_predictions / val_total_predictions\n    \n    # Check if this is the best model so far based on validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_params = model.state_dict().copy()\n        \n    # Print epoch-wise training and validation loss, and accuracy\n    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:14:51.475562Z","iopub.execute_input":"2024-02-25T12:14:51.47651Z","iopub.status.idle":"2024-02-25T12:20:46.756863Z","shell.execute_reply.started":"2024-02-25T12:14:51.476468Z","shell.execute_reply":"2024-02-25T12:20:46.755837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## RESULTS\n# Epoch 1/3, Train Loss: 0.0375, Train Accuracy: 0.9973, Val Loss: 0.0224, Val Accuracy: 0.9964\n# Epoch 2/3, Train Loss: 0.0147, Train Accuracy: 0.9982, Val Loss: 0.0232, Val Accuracy: 0.9964\n# Epoch 3/3, Train Loss: 0.0125, Train Accuracy: 0.9982, Val Loss: 0.0259, Val Accuracy: 0.9964","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:46.758754Z","iopub.execute_input":"2024-02-25T12:20:46.759053Z","iopub.status.idle":"2024-02-25T12:20:46.763299Z","shell.execute_reply.started":"2024-02-25T12:20:46.759028Z","shell.execute_reply":"2024-02-25T12:20:46.762353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertForSequenceClassification\nfrom torch.utils.data import DataLoader\n\n# Instantiate the best model using the best parameters\nbest_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\nbest_model.load_state_dict(best_model_params)\nbest_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:46.764647Z","iopub.execute_input":"2024-02-25T12:20:46.765474Z","iopub.status.idle":"2024-02-25T12:20:47.288595Z","shell.execute_reply.started":"2024-02-25T12:20:46.76544Z","shell.execute_reply":"2024-02-25T12:20:47.287672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Applying preprocessing on texts\ntest_essays['clean_text'] = test_essays['text'].apply(clean_text)\ntexts = test_essays['clean_text'].tolist()\ntokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:47.290415Z","iopub.execute_input":"2024-02-25T12:20:47.290703Z","iopub.status.idle":"2024-02-25T12:20:47.297192Z","shell.execute_reply.started":"2024-02-25T12:20:47.290679Z","shell.execute_reply":"2024-02-25T12:20:47.296349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction on test data using the best model\nwith torch.no_grad():\n    outputs = best_model(**tokenized_texts)\n    logits = outputs.logits\n    \npredictions = torch.softmax(logits, dim=1)[:, 1]","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:47.298355Z","iopub.execute_input":"2024-02-25T12:20:47.298646Z","iopub.status.idle":"2024-02-25T12:20:47.371216Z","shell.execute_reply.started":"2024-02-25T12:20:47.298623Z","shell.execute_reply":"2024-02-25T12:20:47.370131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission DataFrame with essay IDs and corresponding predictions\nsubmission = pd.DataFrame({\n    'id': test_essays['id'],\n    'generated': predictions\n})","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:47.381251Z","iopub.execute_input":"2024-02-25T12:20:47.381739Z","iopub.status.idle":"2024-02-25T12:20:47.388347Z","shell.execute_reply.started":"2024-02-25T12:20:47.381612Z","shell.execute_reply":"2024-02-25T12:20:47.387582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the submission DataFrame to a CSV file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-25T12:20:47.405758Z","iopub.execute_input":"2024-02-25T12:20:47.406056Z","iopub.status.idle":"2024-02-25T12:20:47.414276Z","shell.execute_reply.started":"2024-02-25T12:20:47.406033Z","shell.execute_reply":"2024-02-25T12:20:47.413424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}